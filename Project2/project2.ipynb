{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlsPJUw7wCWZ"
   },
   "source": [
    "# 02807: Project 2\n",
    " \n",
    "## Practical information\n",
    " \n",
    "* This project must be completed in groups of 3 students.\n",
    "    * The group must be registered on the course site on DTU Learn: My Course > Groups\n",
    "    * Groups must be registered anew (even if you already registered for Project 1)\n",
    "* This project must be handed in as a jupyter notebook to the course site on DTU Learn. \n",
    "    * Go to the Course Content > Assignments tab to upload your submission. \n",
    "* This project is due on Monday, November 29, 20:00.\n",
    "\n",
    "## Submission rules\n",
    "\n",
    "* Each group has to hand in *one* notebook (`.ipynb`) with their solutions, including a filled out Contribution table (see below).\n",
    "* Your solution must be written in Python.\n",
    "* For each question you should use the cells provided (\"`# your code goes here`\" and \"*your explanation here*\") for your solution\n",
    "    * It is allowed to add code cells within a question block, but consider if it's really necessary.\n",
    "* You should not remove the problem statements, and you should not modify the structure of the notebook.\n",
    "* Your notebook should be runnable and readable from top to bottom.\n",
    "    * Meaning that your code cells work when run in order (from top to bottom).\n",
    "    * Output of any cell depends only on itself and cells above it.\n",
    "* Your notebook should be submitted after having been run from top to bottom.\n",
    "    * This means outputs are interpretable without necessarily running your cells.\n",
    "    * The simplest way to achieve this is using the jupyter menu item Kernel > Restart & Run All just prior to submission. If any cell fails when you do this, your notebook is not ready for submission.\n",
    "    * Exercise 3 in particular will take time to finish, plan accordingly, that is, make sure you have time to run your notebook from top to bottom.\n",
    "* Failure to comply may make it impossible for us to evaluate your submission properly, which will likely negatively impact the points awarded.\n",
    "\n",
    "## Solution guidelines\n",
    "* Data processing is via Spark for the first three exercises and pandas/SQL in the fourth exercise.\n",
    "* Where naming of dataframes and functions are explicitly stated, these must be used.\n",
    "* Your solutions will be evaluated by correctness, code quality and interpretability of the output. \n",
    "    * You have to write clean, readable and efficient Spark code that will generate sensible execution plans.\n",
    "    * You have to write clean, readable and efficient SQL queries.\n",
    "    * Your tables and visualisations should be meaningful and easy to read. This requires, but is not limited to, including headers, legends and well-written (brief) descriptions for graphs/charts. In this step you've found the data processing solution, so put also some effort into its presentation.\n",
    "\n",
    "## Colaboration policy\n",
    " \n",
    "* It is not allowed to collaborate on the exercises with students outside your group, except for discussing the text of the exercise with teachers and fellow students enrolled on the course in the same semester. \n",
    "* It is not allowed to exchange, hand-over or in any other way communicate solutions or parts of solutions to the exercises. \n",
    "* It is not allowed to use solutions from similar courses, or solutions found elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP30rwxHDQyG"
   },
   "source": [
    "## Contribution table and grading\n",
    "\n",
    "* The total amount of points in the project is 150.\n",
    "* You have to indicate who has solved each part of each exercise in a **contribution table**. \n",
    "* A group member can take credit for solving a part of an exercise only if they have contributed **substantially** to the solution. \n",
    "    * Simple contributions, such as correcting a small bug or double-checking the results of functions, are not sufficient for taking credit for a solution.\n",
    "    * Several group members can take credit for the same solution if they all have contributed substantially to it.\n",
    "* Each group member must contribute **at least 65 points**. \n",
    "    * If no name is provided for an exercise's part, **all group members** are considered contributors to it.\n",
    "* Group members should decide amongst themselves how to collaborate on the project to meet these constraints.  \n",
    "* Scores are individual. The score $\\text{score}(m)$ for a group member $m$ ranges from 0 to 10 and is calculated as follows: \n",
    "\n",
    "  * $\\text{individual-score}(m) = \\frac{\\text{total number of points for the parts correctly solved by }m}{\\text{total number of points for the parts contributed by }m}$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{\\text{total number of points correctly solved by any group member}}{\\text{total number of points in the project}}$\n",
    "\n",
    "  * $\\text{score}(m) =  7.5 \\cdot \\text{individual-score}(m) + 2.5 \\cdot \\text{group-score}$\n",
    "  \n",
    "  \n",
    "* The following is an example of a contributions table:\n",
    "\n",
    "|        | Exercise 1 | Exercise 2 | Exercise 3 | Exercise 4 |\n",
    "|--------|------------|------------|------------|------------|\n",
    "| **Part 1** | John       |    Mary        |     Ann       |   Mary, Ann         |\n",
    "| **Part 2** |     Mary       |    Mary        |   Ann         |    John, Ann        |\n",
    "| **Part 3** |     John, Mary, Ann       |      John, Ann      |   John         | John      |\n",
    "| **Part 4** | Ann       |  Ann          |     John, Mary       | John       |\n",
    "| **Part 5** | **n.a.**     | John, Mary, Ann           | **n.a.**       | **n.a.**       |\n",
    "\n",
    "\n",
    "* **Example**: in the contribution table above, suppose that all parts are solved correctly except for those of Exercise 4 which are all wrong. Then Ann's score is calculated as follows:\n",
    "\n",
    "  * $\\text{individual-score}(Ann) = \\frac{5+5+10+5+5+15+15}{5+5+10+5+5+15+15+15+5} = \\frac{60}{80} = 0.75$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{95}{150} = 0.633$\n",
    "\n",
    "  * $\\text{score}(Ann) = 7.5\\cdot 0.75 + 2.5 \\cdot 0.633 = 7.21$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vklet8XdRGVV"
   },
   "source": [
    "# Group contribution table \n",
    "\n",
    "This table must be filled before submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:49.385821Z",
     "start_time": "2021-11-02T13:07:48.995596Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "chiXA3CzRSA1",
    "outputId": "752ba847-32fb-48e7-a4ea-2ce363e3a706"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exercise 1</th>\n",
       "      <th>Exercise 2</th>\n",
       "      <th>Exercise 3</th>\n",
       "      <th>Exercise 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Part 1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Part 2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Part 3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Part 4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Part 5</td>\n",
       "      <td>n.a</td>\n",
       "      <td></td>\n",
       "      <td>n.a</td>\n",
       "      <td>n.a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Exercise 1 Exercise 2 Exercise 3 Exercise 4\n",
       "Part 1                                            \n",
       "Part 2                                            \n",
       "Part 3                                            \n",
       "Part 4                                            \n",
       "Part 5        n.a                   n.a        n.a"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'Exercise 1' : ['', '', '', '', 'n.a'], \n",
    "     'Exercise 2' : ['', '', '', '', ''],\n",
    "     'Exercise 3' : ['', '', '', '', 'n.a'],\n",
    "     'Exercise 4' : ['', '', '', '', 'n.a'],\n",
    "     } \n",
    "  \n",
    "ct = pd.DataFrame(d, index=['Part 1', 'Part 2', 'Part 3', 'Part 4', 'Part 5']) \n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPlNQTICo9sH"
   },
   "source": [
    "# The AirBnB dataset\n",
    "\n",
    "<img src=\"https://www.esquireme.com/public/images/2019/11/03/airbnb-678x381.jpg\" alt=\"airbnb\" width=\"400\"/>\n",
    "\n",
    "[Airbnb](http://airbnb.com) is an online marketplace for arranging or offering lodgings. In the first three exercises you will use Spark to analyze data obtained from the Airbnb website (stricly speaking via data scraped by [insideairbnb](http://insideairbnb.com/get-the-data.html)). The purpose of your analysis is to extract insights about listings as a whole, specifics about London, and sentiment analysis of reviews (word positivity).\n",
    "\n",
    "\n",
    "## Loading data\n",
    "The dataset consists of listings (offered lodgings) and reviews (submitted by users). The `.csv`'s you'll work with vary between the first three exercises, but is structured so that the function below will load it into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:51.283648Z",
     "start_time": "2021-11-02T13:07:51.280806Z"
    },
    "id": "E9BEj7re4jnN"
   },
   "outputs": [],
   "source": [
    "def load_csv_as_dataframe(path):\n",
    "    return spark.read.option('header', True) \\\n",
    "                .option('inferSchema', True) \\\n",
    "                .option('multiLine', 'True') \\\n",
    "                .option('escape', '\"') \\\n",
    "                .option('mode', 'DROPMALFORMED')\\\n",
    "                .csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4hk-43Io9sI"
   },
   "source": [
    "## Imports and Spark session\n",
    "\n",
    "* You'll need to adapt the `JAVA_HOME` environment variable to your setup. \n",
    "* You should set the `spark.driver.memory` value to the amount of memory on your machine. \n",
    "* It may be required for you to install some of the packages imported below (e.g. pandasql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:32:31.290655Z",
     "start_time": "2021-11-02T13:32:31.284171Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy1rMfRh4jnN",
    "outputId": "bdfabe69-f041-4b5d-cbf5-7c14679e149a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyspark) (0.10.9.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandasql==0.7.3 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandasql==0.7.3) (1.21.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandasql==0.7.3) (0.25.1)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandasql==0.7.3) (1.4.27)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->pandasql==0.7.3) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->pandasql==0.7.3) (2019.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sqlalchemy->pandasql==0.7.3) (1.1.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sqlalchemy->pandasql==0.7.3) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->pandasql==0.7.3) (1.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->sqlalchemy->pandasql==0.7.3) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Instructions on p. 20 Learning Spark, 2nd ed.\n",
    "# Here's a quick-guide, googling may also be required\n",
    "# 1) Install pyspark via conda/pip\n",
    "#          pyspark requires the JAVA_HOME environment variable is set.\n",
    "# 2) Install JDK 8 or 11, figure out the install location\n",
    "#          Suggest to use https://adoptopenjdk.net/\n",
    "# 3) Update the JAVA_HOME environment variable set programmatically below \n",
    "#    with your install location specifics\n",
    "\n",
    "# JAVA_HOME environment variable is set programatically below\n",
    "# but you must point it to your local install\n",
    "\n",
    "import os\n",
    "# os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/'\n",
    "!pip install pyspark\n",
    "\n",
    "!pip install -U -q PyDrive\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq\n",
    "!pip install pandasql==0.7.3\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "# If you get \"Job aborted due to stage failure\" and \n",
    "# \"Python worker failed to connect back.\" exceptions, \n",
    "# this should be solved by additionally setting these \n",
    "# environment variables\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:53.807655Z",
     "start_time": "2021-11-02T13:07:52.901610Z"
    },
    "id": "2ftW7yaGo9sJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB)\n",
      "Requirement already satisfied: markupsafe~=2.0.1 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas-profiling) (2.0.1)\n",
      "Collecting joblib~=1.0.1\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3\n",
      "  Using cached pandas-1.3.4-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas-profiling) (6.0)\n",
      "Collecting scipy>=1.4.1\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-win_amd64.whl (34.1 MB)\n",
      "Collecting htmlmin>=0.1.12\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tangled-up-in-unicode==0.1.0\n",
      "  Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: requests>=2.24.0 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas-profiling) (2.26.0)\n",
      "Collecting missingno>=0.4.2\n",
      "  Downloading missingno-0.5.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting seaborn>=0.10.1\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Collecting multimethod>=1.4\n",
      "  Downloading multimethod-1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting pydantic>=1.8.1\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas-profiling) (3.0.1)\n",
      "Collecting matplotlib>=3.2.0\n",
      "  Downloading matplotlib-3.5.0-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas-profiling) (1.17.2)\n",
      "Collecting phik>=0.11.1\n",
      "  Downloading phik-0.12.0-cp37-cp37m-win_amd64.whl (660 kB)\n",
      "Collecting tqdm>=4.48.2\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting visions[type_image_path]==0.7.4\n",
      "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "Collecting networkx>=2.4\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (21.2.0)\n",
      "Collecting imagehash\n",
      "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting Pillow\n",
      "  Downloading Pillow-8.4.0-cp37-cp37m-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (21.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.2-py3-none-any.whl (880 kB)\n",
      "Collecting setuptools-scm>=4\n",
      "  Downloading setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.2)\n",
      "Collecting numpy>=1.16.0\n",
      "  Using cached numpy-1.21.4-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2019.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pydantic>=1.8.1->pandas-profiling) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm>=4.48.2->pandas-profiling) (0.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.2.0->pandas-profiling) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->pandas-profiling) (40.8.0)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Collecting PyWavelets\n",
      "  Downloading PyWavelets-1.2.0-cp37-cp37m-win_amd64.whl (4.2 MB)\n",
      "Using legacy 'setup.py install' for htmlmin, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for imagehash, since package 'wheel' is not installed.\n",
      "Installing collected packages: tomli, setuptools-scm, Pillow, numpy, fonttools, tangled-up-in-unicode, scipy, PyWavelets, pandas, networkx, multimethod, matplotlib, visions, seaborn, joblib, imagehash, tqdm, pydantic, phik, missingno, htmlmin, pandas-profiling\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.17.2\n",
      "    Uninstalling numpy-1.17.2:\n",
      "      Successfully uninstalled numpy-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\users\\\\minki.000\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29744/320088377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install pandas-profiling'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandasql\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpsql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "!pip install pandas-profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:57.883383Z",
     "start_time": "2021-11-02T13:07:53.808476Z"
    },
    "id": "OBILY9sf4jnN"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29744/719000487.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local[*]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mc:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;31m# preexec_fn not supported on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\minki.000\\appdata\\local\\programs\\python\\python37\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "# Sets memory limit on driver and to use all CPU cores\n",
    "conf = SparkConf().set('spark.ui.port', '4050') \\\n",
    "        .set('spark.driver.memory', '4g') \\\n",
    "        .setMaster('local[*]')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:58.627425Z",
     "start_time": "2021-11-02T13:07:57.884329Z"
    },
    "id": "cJpOoxzN4jnN",
    "outputId": "517231b3-51f5-4e7f-9481-8384d0989a00"
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:58.667031Z",
     "start_time": "2021-11-02T13:07:58.628174Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5lJlshk4jnN",
    "outputId": "07019bc8-17e7-4a9f-9921-7c3979b41796"
   },
   "outputs": [],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PdveMrU4jnO"
   },
   "source": [
    "# Exercise 1: Listings and cities (20 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. \n",
    "* For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. \n",
    "* For parts requiring visualisation, the `toPandas` call should be followed only by functions necessary to customize the plotting/layout steps (i.e. no data processing take place after your spark dataframe is materialized).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5LAuprs4jnO"
   },
   "source": [
    "## Part 1: Preparing the dataframe (5 pts)\n",
    "\n",
    "Your data source is [this zip archive](https://data-download.compute.dtu.dk/c02807/listings.csv.zip) which you must uncompress and place in the same folder as this notebook. It is loaded in the next cell and named `df_listings`.\n",
    "\n",
    "After the data is read, you should select the columns necessary for exercise 1, 2 and 3 (by reading ahead or iteratively extend this loading code). Name this dataframe `df_listings_analysis` and make use of caching.\n",
    "\n",
    "Prices are in local currency, but are nonetheless prefixed with `$` and contains thousands separator commas. You will need to remove these characters and cast the price column to `pyspark.sql.types.DoubleType`. Observe that if this casting is not possible, the result of the cast is `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:29:07.681609Z",
     "start_time": "2021-10-27T15:28:34.756446Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "NTybzL654jnO",
    "outputId": "e5761074-f16d-4e44-c872-5e701ae4304e"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a0d92380f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_listings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_as_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:/Users/Minki.000/Desktop/listings.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-1d37222733e2>\u001b[0m in \u001b[0;36mload_csv_as_dataframe\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv_as_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inferSchema'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiLine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'escape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DROPMALFORMED'\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"C\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df_listings = load_csv_as_dataframe('listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:36.175515Z",
     "start_time": "2021-10-31T14:35:36.170621Z"
    },
    "id": "qnbDRPXH4jnO"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QZ0w17Y4jnO"
   },
   "source": [
    "## Part 2: Listing and neighbourhood counts (5 pts)\n",
    "\n",
    "Compute and visualise the number of listings and the number of different neighbourhoods per city, restricted to the 15 cities having the most listings. The x-axis should be ordered by number of listings (high to low).\n",
    "\n",
    "Make sure to use the `neighbourhood_cleansed` column in your computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:44.882818Z",
     "start_time": "2021-10-31T14:35:44.880268Z"
    },
    "id": "kxo1ZH_q4jnO"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9BFleEJ4jnO"
   },
   "source": [
    "## Part 3: Price averages (5 pts)\n",
    "\n",
    "Compute and visualise the average price of listings per city, restricted to the 15 cities having the most listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:50.717376Z",
     "start_time": "2021-10-31T14:35:50.714248Z"
    },
    "id": "nfggw4Sp4jnO"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIpHBUBk4jnO"
   },
   "source": [
    "## Part 4: Value for money (5 pts)\n",
    "\n",
    "The value of a listing is its rating divided by its price. The value of a city is the average value of its listings. \n",
    "\n",
    "Prices are only comparable when the local currency is the same. We'll therefore consider a subset of Euro-zone cities as defined in `eurozone_cities`.\n",
    "\n",
    "Compute and visualise the value per city, restricted to the Euro-zone cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:29:53.566725Z",
     "start_time": "2021-10-27T15:29:53.565232Z"
    },
    "id": "wqspMVZ24jnO"
   },
   "outputs": [],
   "source": [
    "eurozone_cities = [\n",
    "    'Paris', 'Roma', 'Berlin', 'Madrid', 'Amsterdam', 'Barcelona', 'Milano', 'Lisboa',\n",
    "    'München', 'Wien', 'Lyon', 'Firenze', 'Porto', 'Napoli', 'Bordeaux', 'Venezia',\n",
    "    'Málaga', 'Sevilla', 'València'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:57.257011Z",
     "start_time": "2021-10-31T14:35:57.253676Z"
    },
    "id": "YtktiltR4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbJdSKC84jnP"
   },
   "source": [
    "# Exercise 2: The case of London (30 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. \n",
    "* For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. \n",
    "* For parts requiring visualisation, the `toPandas` call should be followed only by functions necessary to customize the plotting/layout steps (i.e. no data processing take place after your spark dataframe is materialized). \n",
    "* You may need multiple queries to solve the individual parts.\n",
    "\n",
    "Your dataframe is a subset of `df_listings_analysis` and should be named `df_listings_london`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:05.783859Z",
     "start_time": "2021-10-31T14:36:05.781830Z"
    },
    "id": "aeVpGvZF4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-KJqz1H4jnP"
   },
   "source": [
    "## Part 1: Price distribution (5 pts)\n",
    "\n",
    "Compute and visualise the distribution of prices, for all prices up to and including the 95-percentile. Additionally, compute and visualise the distribution of prices, for all prices above the 95-percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:15.616156Z",
     "start_time": "2021-10-31T14:36:15.613196Z"
    },
    "id": "kWc-mqph4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Oa37Ck4jnP"
   },
   "source": [
    "## Part 2: Prices by type of property (5 pts)\n",
    "\n",
    "Compute and visualise the average price and average rating per type of property, for property types with 75 or more listings. \n",
    "\n",
    "Your visualisation should be a single bar chart with two y-axes and two bars per property type. The x-axis should be ordered by average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:21.187516Z",
     "start_time": "2021-10-31T14:36:21.179099Z"
    },
    "id": "b5jx-W9O4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvUyAWA94jnP"
   },
   "source": [
    "## Part 3: Best offering in the neighbourhood (10 pts)\n",
    "\n",
    "The value of a listing is its rating divided by its price. Compute and display a dataframe (with the columns you selected in Exercise 1 and those computed in this part) with the 3 highest valued listings in each neighbourhood, and having a value above 5. Make sure to use the `neighbourhood_cleansed` column in your computations.\n",
    "\n",
    "Computing ranks based on value can be achieved using `pyspark.sql.window.Window`. This may produce equal ranks (i.e. when the value of two listings are the same).\n",
    "\n",
    "Remember to use `pd.set_option('display.max_rows', <n>)` with appropriate `<n>` so all rows are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:05.393213Z",
     "start_time": "2021-10-27T15:30:03.259525Z"
    },
    "id": "8lmAGdHw4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZdZ5-FE4jnQ"
   },
   "source": [
    "## Part 4: Activity by month (5 pts)\n",
    "\n",
    "Activity is given by the number of reviews received in a given time period. Compute and visualise the activity based on month, that is, the total number of reviews given in January, February, etc..\n",
    "\n",
    "Your additional data source is [this zip archive](https://data-download.compute.dtu.dk/c02807/reviews_london.csv.zip) which you must uncompress and place in the same folder as this notebook. It is loaded in the next cell and named `df_reviews_london`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:07.720386Z",
     "start_time": "2021-10-27T15:30:05.404386Z"
    },
    "id": "MGtA6uzq4jnQ"
   },
   "outputs": [],
   "source": [
    "df_reviews_london = load_csv_as_dataframe('reviews_london.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:42.395920Z",
     "start_time": "2021-10-31T14:36:42.393979Z"
    },
    "id": "NohJUbEg4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So8nbHnR4jnQ"
   },
   "source": [
    "## Part 5: Reviews per listing (5 pts)\n",
    "\n",
    "Each London listing has received 0 or more reviews. \n",
    "\n",
    "Display a dataframe showing 1) The number of listings, 2) The average number of reviews a listing receives, 3) The standard deviation of the reviews per listing distribution, 4) The minimum number of reviews any listing has received, and 5) The maximum number of reviews any listing has received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:48.293254Z",
     "start_time": "2021-10-31T14:36:48.291307Z"
    },
    "id": "xnH01VPh4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62poLc34jnQ"
   },
   "source": [
    "# Exercise 3: Word sentiment (45 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. You may need multiple queries to solve the individual parts.\n",
    "\n",
    "The goal here is to determine what sentiment (positive or negative) words in reviews have. Roughly speaking, we want each word to be assigned a score based on the rating of the reviews in which the word occurs in the review comment. We'd expect words such as \"clean\", \"comfortable\", \"superhost\" to receive high scores, while words such as \"unpleasant\", \"dirty\", \"disgusting\" would receive low scores.\n",
    "\n",
    "As individual reviews do not have a rating, we'll consider the rating of individual reviews to be the rating of its related listing (i.e. assuming each review gave the average rating (`review_scores_rating`) of the listing). \n",
    "\n",
    "The score of a word is given by the mean review rating over the reviews in which that word occurs in the comment. We require words to appear in at least 0.5% (1 in 200) listings, and to be at least 4 characters, for it to have a defined score.\n",
    "\n",
    "Formally, when a word $w$ occurs in at least $0.5\\%$ of listings and $|w| > 3$, its score is\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "score(w) = \\frac{1}{|C_w|}\\sum_{comment \\in C_w} \\text{review_rating}(comment)\n",
    "\\end{align*}\n",
    "$\n",
    ", where \n",
    "* $C_w = \\{comment \\mid w \\text{ occurs in } \\text{clean_text}(comment)\\}$, the set (so no duplicates) of comments in which $w$ occurs, and\n",
    "* $\\text{clean_text}(comment)$ is the result of your `clean_text` function defined below, and\n",
    "* $\\text{review_rating}(comment)$ is the `review_scores_rating` of the listing which this $comment$ is related to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFNxM4PS4jnQ"
   },
   "source": [
    "## Part 1: Toy data (15 pts)\n",
    "\n",
    "To get started we'll consider a toy example where the input is `df_sentiment_listings_toy` and `df_sentiment_reviews_toy` defined in the next code cell. You should provide an implementation of `calculate_word_scores_toy` in the subsequent code cell. Your implementation should result in a query that when given the toy example dataframes as input and is materialized with `toPandas()` produces this table:\n",
    "\n",
    "|    | word   |   word_score |   listing_occurences |   word_occurences |   comment_occurences |\n",
    "|---:|:-------|-------------:|---------------------:|------------------:|---------------------:|\n",
    "|  0 | aaaa   |      7       |                    3 |                 5 |                    5 |\n",
    "|  1 | bbbb   |      6.66667 |                    2 |                 3 |                    3 |\n",
    "|  2 | eeee   |      0       |                    1 |                 1 |                    1 |\n",
    "|  3 | dddd   |      5       |                    1 |                 1 |                    1 |\n",
    "|  4 | cccc   |      5       |                    2 |                 2 |                    2 |'\n",
    "\n",
    "Observe that `word_occurences` and `comment_occurences` are the same as words occuring multiple times in a comment are counted once, and that `clean_text` is used to ignore casing and discard non-words. Additionally, any word occuring at least once will occur in more than 1 out of 200 listings on this toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:20.311738Z",
     "start_time": "2021-10-27T15:30:20.131208Z"
    },
    "id": "p2_01YSQ4jnQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema_listings = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('review_scores_rating', StringType(), True),\n",
    "])\n",
    "data_listings = [\n",
    "    {'id': '0', 'review_scores_rating': '10'},\n",
    "    {'id': '1', 'review_scores_rating': '5'},\n",
    "    {'id': '2', 'review_scores_rating': '0'},\n",
    "]\n",
    "df_sentiment_listings_toy = spark.createDataFrame(data_listings, schema_listings)\n",
    "\n",
    "schema_reviews = StructType([\n",
    "    StructField('listing_id', StringType(), True),\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('comments', StringType(), True),\n",
    "])\n",
    "data_reviews = [\n",
    "    {'listing_id': '0', 'id': '100', 'comments': 'aaaa bbbb          cccc'},\n",
    "    {'listing_id': '0', 'id': '101', 'comments': 'aaaa bbbb '},\n",
    "    {'listing_id': '0', 'id': '102', 'comments': 'aaaa aAAa          aaaa'},\n",
    "    {'listing_id': '1', 'id': '103', 'comments': 'Aaaa bbb ccc'},\n",
    "    {'listing_id': '1', 'id': '104', 'comments': 'dddd %ˆ&*'},\n",
    "    {'listing_id': '2', 'id': '105', 'comments': 'AaaA'},\n",
    "    {'listing_id': '2', 'id': '106', 'comments': 'bbbb ccc e&eˆˆee'},\n",
    "    {'listing_id': '2', 'id': '107', 'comments': 'cccc cccc'},\n",
    "]\n",
    "\n",
    "df_sentiment_reviews_toy = \\\n",
    "    spark.createDataFrame(data_reviews, schema_reviews) \\\n",
    "        .select(F.col('listing_id'), F.col('id').alias('comment_id'), F.col('comments'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:24.249616Z",
     "start_time": "2021-10-27T15:30:20.312564Z"
    },
    "id": "ybRkPjCD4jnR"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "def clean_text(col):\n",
    "    \"\"\"\n",
    "        Cleans the text (comment) associated with col. The\n",
    "        cleaning should:\n",
    "            1) Lower case the text\n",
    "            2) Turn multiple whitespaces into single whitespaces\n",
    "            3) Remove anything but letters, digits and whitespaces\n",
    "        \n",
    "        :col: A Spark Column object containing text data\n",
    "        :returns: A Spark Column object.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def calculate_word_scores_toy(df_list, df_rev):\n",
    "    \"\"\"\n",
    "        Calculates the word score over listings in df_list and\n",
    "        reviews in df_rev. The table produced should have the \n",
    "        same columns as specified in part 1.\n",
    "        \n",
    "        :returns: A pandas DataFrame\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "calculate_word_scores_toy(df_sentiment_listings_toy, df_sentiment_reviews_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF0-Et6Y4jnR"
   },
   "source": [
    "## Part 2: London comments (15 pts)\n",
    "\n",
    "In this part we'll calculate word scores for the comments related to London listings only. You should implement `count_relevant_listings` and `calculate_word_scores` (it will be an extension of your function from part 1) below. See the mathematical definition and docstrings for intended behaviour.\n",
    "\n",
    "The function `calculate_word_scores` should return the top 10 and bottom 10 words by score. You should **not** use caching in your function.\n",
    "\n",
    "Make sure your satisfy all conditions for a word to be scored (e.g. correctly calculating how many total listings scores are computed over). You should also consider whether your query is optimally structured in terms of computation time. Moreover, `pd.set_option('display.max_rows', <n>)` should be set with sufficiently high `n` to show all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T13:35:09.413747Z",
     "start_time": "2021-10-31T13:35:09.386493Z"
    },
    "id": "NL05dnDH4jnR"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "def count_relevant_listings(df_list, df_rev):\n",
    "    \"\"\"\n",
    "        Calculates the number of listings in df_list that has a \n",
    "        review in df_rev. A listing that is reviewed more than once\n",
    "        should only count as one.\n",
    "        \n",
    "        :returns: An integer \n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def calculate_word_scores(df_list, df_rev, listings_count):\n",
    "    \"\"\"\n",
    "        Calculates the word score over listings in df_list and\n",
    "        reviews in df_rev. The value of listings_count should \n",
    "        be used to filter out words not occuring frequently enough\n",
    "        in comments. The table produced should have the same columns\n",
    "        as in part 1 of this exercise.\n",
    "        \n",
    "        :returns: A pandas DataFrame containing the top 10 and \n",
    "        bottom 10 words based on their word score, sorted by word_score.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T13:37:21.918401Z",
     "start_time": "2021-10-31T13:35:10.034363Z"
    },
    "id": "ZoUPZ6F-4jnR"
   },
   "outputs": [],
   "source": [
    "# should not be modified\n",
    "from IPython.display import display\n",
    "\n",
    "relevant_listings_count_london = count_relevant_listings(df_listings_london, df_reviews_london)\n",
    "word_scores_london_timing = %timeit -o -n1 -r1 display( \\\n",
    "    calculate_word_scores(df_listings_london, \\\n",
    "                          df_reviews_london, \\\n",
    "                          relevant_listings_count_london) \\\n",
    ")\n",
    "\n",
    "word_scores_london_timing.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAPvcNbU4jnR"
   },
   "source": [
    "## Part 3: Scalability (10 pts)\n",
    "\n",
    "The listings from London make up a little less than 2% of the entire set of listings. In this part we're interested in how the amount of input data impacts computation time, that is, how `calculate_word_scores` scales as data increases. To this end, we've made multiple samples of the dataset of varying sizes.\n",
    "\n",
    "The experiment reuses `count_relevant_listings` and `calculate_word_scores` that you implemented in part 2. Code needed for this part is provided to you. \n",
    "\n",
    "Your task is to obtain the data sources, run the code cells below, and explain the results you get. Specifically, you must explain any non-linear relationship between data size and computation time, using the markdown cell at the end of this part. In finding explanations, using the Spark UI to investigate the anatomy of your queries may prove valuable. Once you've found an explanation, state a potential solution to remedy the issue. Lastly, include a paragraph stating the specifications of your computer hardware (memory, CPU cores and clock speed, solid state disk or not) on which the experiment has been run.\n",
    "\n",
    "*Implementation note* Make sure you've properly configured `spark.driver.memory` (it requires a kernel restart to update the value). It may be that your query fails on the larger samples due to running out of compute resources. This is likely caused by a suboptimal `calculate_word_scores`, but can be from reaching the limits of your hardware. If you think the latter is the case, argue for this perspective in the markdown cell.\n",
    "\n",
    "Your data sources are (uncompress and place in the same directory as this notebook):\n",
    "* 0.25%: [listings](https://data-download.compute.dtu.dk/c02807/listings_0-dot-25percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_0-dot-25percent.csv.zip)\n",
    "* 0.5%: [listings](https://data-download.compute.dtu.dk/c02807/listings_0-dot-5percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_0-dot-5percent.csv.zip)\n",
    "* 1%: [listings](https://data-download.compute.dtu.dk/c02807/listings_1-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_1-dot-0percent.csv.zip)\n",
    "* 2%: [listings](https://data-download.compute.dtu.dk/c02807/listings_2-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_2-dot-0percent.csv.zip)\n",
    "* 4%: [listings](https://data-download.compute.dtu.dk/c02807/listings_4-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_4-dot-0percent.csv.zip)\n",
    "* 8%: [listings](https://data-download.compute.dtu.dk/c02807/listings_8-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_8-dot-0percent.csv.zip)\n",
    "* 12.5%: [listings](https://data-download.compute.dtu.dk/c02807/listings_12-dot-5percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_12-dot-5percent.csv.zip)\n",
    "* 16%: [listings](https://data-download.compute.dtu.dk/c02807/listings_16-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_16-dot-0percent.csv.zip)\n",
    "* 25%: [listings](https://data-download.compute.dtu.dk/c02807/listings_25-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_25-dot-0percent.csv.zip)\n",
    "* 50%: [listings](https://data-download.compute.dtu.dk/c02807/listings_50-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_50-dot-0percent.csv.zip)\n",
    "* 75%: [listings](https://data-download.compute.dtu.dk/c02807/listings_75-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_75-dot-0percent.csv.zip)\n",
    "* 100%: [listings](https://data-download.compute.dtu.dk/c02807/listings_100-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_100-dot-0percent.csv.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T23:46:48.667540Z",
     "start_time": "2021-10-28T23:46:48.663425Z"
    },
    "id": "u9BPHTsQ4jnR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def calculate_word_scores_timed(percent_str):\n",
    "    \"\"\"\n",
    "        Calculates word scores over a sampled dataset indicated\n",
    "        by percent_str.\n",
    "        \n",
    "        :returns: A dictionary with benchmarking information and\n",
    "        the calculated values.\n",
    "    \"\"\"\n",
    "    df_listings = load_csv_as_dataframe(f'listings_{percent_str}percent.csv')\n",
    "    df_reviews = load_csv_as_dataframe(f'reviews_{percent_str}percent.csv')\n",
    "    \n",
    "    listings_count = count_relevant_listings(df_listings, df_reviews)\n",
    "\n",
    "    start = time.time()\n",
    "    df_word_scores = calculate_word_scores(df_listings, df_reviews, listings_count)\n",
    "    end = time.time()\n",
    "    return {\n",
    "        'percentage': float(percent_str.replace('-dot-', '.')), \n",
    "        'time_spent': f\"{end - start:.2f}\", \n",
    "        'relevant_listings': listings_count, \n",
    "        'df': df_word_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.491711Z",
     "start_time": "2021-10-28T23:46:49.516730Z"
    },
    "id": "0eutRq674jnR"
   },
   "outputs": [],
   "source": [
    "data_percentages = [\n",
    "    '0-dot-25', '0-dot-5', '1-dot-0', '2-dot-0', '4-dot-0', '8-dot-0',\n",
    "    '12-dot-5', '16-dot-0', '25-dot-0'\n",
    "]\n",
    "score_data = {\n",
    "    percentage_str: calculate_word_scores_timed(percentage_str) for percentage_str in data_percentages\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.502845Z",
     "start_time": "2021-10-29T00:35:02.493477Z"
    },
    "id": "6gn8hhmq4jnR"
   },
   "outputs": [],
   "source": [
    "score_data['50-dot-0'] = calculate_word_scores_timed('50-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.506905Z",
     "start_time": "2021-10-29T00:35:02.503911Z"
    },
    "id": "o6S0UVl74jnS"
   },
   "outputs": [],
   "source": [
    "score_data['75-dot-0'] = calculate_word_scores_timed('75-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.511277Z",
     "start_time": "2021-10-29T00:35:02.510002Z"
    },
    "id": "Co5DP5jH4jnS"
   },
   "outputs": [],
   "source": [
    "score_data['100-dot-0'] = calculate_word_scores_timed('100-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.580856Z",
     "start_time": "2021-10-29T00:35:02.512026Z"
    },
    "id": "KXicfeA-4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scores_scaling = pd.DataFrame(score_data).T.convert_dtypes()\n",
    "df_scores_scaling.time_spent = df_scores_scaling.time_spent.astype(float)\n",
    "\n",
    "# Access to word scores of 2 percent data: df_scores_scaling.loc['2-dot-0'].df\n",
    "df_scores_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.872708Z",
     "start_time": "2021-10-29T00:35:02.582105Z"
    },
    "id": "pEviNX0y4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "lower_range = ['0-dot-25', '0-dot-5', '1-dot-0', '2-dot-0', '4-dot-0', '8-dot-0', '16-dot-0']\n",
    "df_scores_scaling[df_scores_scaling.index.isin(lower_range)] \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[0], style='-o', title='Lower range')\n",
    "df_scores_scaling[~df_scores_scaling.index.isin(lower_range)] \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[1], style='-o', title='Upper range')\n",
    "_ = df_scores_scaling \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[2], style='-o', title='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf5c4daR4jnS"
   },
   "source": [
    "*Your explanation to the questions outlined at the start of this part goes here. Make sure you've addressed all questions asked.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-61VA-34jnS"
   },
   "source": [
    "## Part 4: Robustness (5 pts)\n",
    "\n",
    "In this part we'll explore robustness of our word scores, using the values we computed in part 3. We'll do so by comparing top/bottom words for three different samples of the dataset. Specifically, the scores from your maximum (e.g. 100%) computed sample are to be compared with the 12.5% and 2.0% scores.\n",
    "\n",
    "Compute and display a dataframe that accounts for any word found in either of the three samples' top/bottom words, and additionally shows the related `word_score` and `word_occurences` values.\n",
    "\n",
    "Note that `df_scores_scaling.loc['100-dot-0'].df` provides the word scores dataframe of the 100% sample (similarly for the other two). For this part you should rely on pandas functionality only.  Moreover, `pd.set_option('display.max_rows', <n>)` should be set with sufficiently high `n` to show all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:37:40.614597Z",
     "start_time": "2021-10-31T14:37:40.612561Z"
    },
    "id": "9jrmvVmM4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQyEFla44jnS"
   },
   "source": [
    "# Exercise 4: Transactions analysis (55 pts)\n",
    "\n",
    "In this exercise the goal is to analyse historical business transactions (sales of parts to other companies), and derive insights about both products and customers.\n",
    "\n",
    "The company X produces and globally sells gadget parts to a number of other companies. You requested the sales department of X to provide you with access to the customer and sales transactions database. To your horror, you've found no such database exists, but the data is instead manually maintained in a spreadsheet (error-prone solution). Intrepid as you are, you've accepted to receive the spreadsheet data as a `.csv`, realizing already data cleaning will be necessary.\n",
    "\n",
    "Your first step (parts 1 and 2) is to clean the data after which you will derive insights about X's business operations (parts 3 and 4).\n",
    "\n",
    "The input data is available here: [transactions.csv](http://courses.compute.dtu.dk/02807/2021/projects/project2/transactions.csv)\n",
    "\n",
    "**Using SQL**\n",
    "\n",
    "In this final exercise you must write SQL to do the data processing in parts 3 and 4. This entails using `psql.sqldf` to execute your queries (up against `df_transactions_cleaned`) which will return a pandas dataframe. Each question should be answered with a *single* query. For visualisation the `psql.sqldf` call should be followed only by functions necessary to customize the plotting/layout steps or reshape the dataframe (i.e. no data processing take place after your SQL statement is materialized as a pandas dataframe).\n",
    "\n",
    "In part 1 and 2 of this exercise, you should make use of pandas functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM2KEdqu4jnS"
   },
   "source": [
    "## Part 1: Data cleaning (15 pts)\n",
    "\n",
    "For each column in the dataframe, investigate and **correct** problematic aspects such as,\n",
    "* Missing values: Insert meaningful values (data imputation). Detectable as `np.nan`'s. A typical value for imputation is the *mode* (most frequent value) of the distribution. If no proper data imputation is possible, you may resort to dropping rows.\n",
    "* Incorrect values: Typos and other data mishaps are present as values are manually entered. Detectable as low-prevalence categorical values, or ambigious data links (e.g. company listed in multiple countries). If no proper value correction is possible, you may resort to dropping rows.\n",
    "\n",
    "In both cases, your strategy for replacing values should be data-driven, that is, shaped by the patterns you observe in the data. It is allowed to skip correcting the data (and instead drop the rows) if few rows are improved by your corrections. If in doubt, do the correction.\n",
    "\n",
    "After all your cleaning steps are completed, you should run the `PandasProfiler` on your cleaned dataset, which should now contain 0% missing cells. Lastly, summarize the issues you identified and how you addressed them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9EAimzb4jnS"
   },
   "source": [
    "### Read, profile and explain\n",
    "\n",
    "As the first step, load the data naming the dataframe `df_transactions`, and make a copy named `df_transactions_cleaned` on which your data cleaning steps will be done. Establish an overview using `PandasProfiler` (but realize there's more to cleaning than what this tool will let you know). Write a paragraph on what the data is about (e.g. what does a row constitute), and a paragraph on what the profile report tells you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:37:53.403596Z",
     "start_time": "2021-10-31T14:37:53.400458Z"
    },
    "id": "A4gokJNI4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KwNFkhZ4jnT"
   },
   "source": [
    "*Your explanation here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uCcxzh54jnT"
   },
   "source": [
    "### Country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:10.817218Z",
     "start_time": "2021-10-31T14:38:10.814361Z"
    },
    "id": "UQobGiSQ4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnxk7N984jnT"
   },
   "source": [
    "### Company column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:23.615156Z",
     "start_time": "2021-10-31T14:38:23.612981Z"
    },
    "id": "Z1vr1z1V4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzkzURFj4jnT"
   },
   "source": [
    "### City column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.402705Z",
     "start_time": "2021-10-27T18:56:56.401612Z"
    },
    "id": "zOQjBe-94jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN671FEX4jnT"
   },
   "source": [
    "### Parts column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.644628Z",
     "start_time": "2021-10-27T18:56:56.643615Z"
    },
    "id": "nMHzx5rl4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stk-AVkM4jnT"
   },
   "source": [
    "### Price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.662243Z",
     "start_time": "2021-10-27T18:56:56.660643Z"
    },
    "id": "bGb4G97k4jnU"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9TgwrNZ4jnU"
   },
   "source": [
    "### Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:43.270635Z",
     "start_time": "2021-10-31T14:38:43.268585Z"
    },
    "id": "am9CNN594jnU"
   },
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ik77rvq4jnU"
   },
   "source": [
    "### Profile `df_transactions_cleaned` and summarize corrections made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:52.078805Z",
     "start_time": "2021-10-31T14:38:52.076381Z"
    },
    "id": "vOuZ6QGY4jnU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTnK1fMF4jnU"
   },
   "source": [
    "*Your summary goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVEOeN4C4jnU"
   },
   "source": [
    "## Part 2: Standardise prices (5 pts)\n",
    "\n",
    "Transaction prices are recorded in the local currency of the client (EUR, GBP, USD or JPY). You will need to convert these prices from local currency into the common currency (chosen here as) EUR, for comparability. These standardised prices should be added as a column to the dataframe called `prices_euro`.\n",
    "\n",
    "Consider a two step process where you 1) Identify what currency has been used, and 2) Calculate the price conversion. Step 1 may reveal the data is still not completely clean (so either correct by impute or drop). For Step 2 look up exchange rates on the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:58.558049Z",
     "start_time": "2021-10-27T18:56:58.556556Z"
    },
    "id": "VTFTFPMX4jnU"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yiqzq3L4jnU"
   },
   "source": [
    "## Part 3: Business insights (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L019f5Sb4jnV"
   },
   "source": [
    "### Company by revenue\n",
    "\n",
    "The revenue of a company is its total value of orders, all time. Compute and visualise all companies by revenue in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:13.954452Z",
     "start_time": "2021-10-31T14:39:13.952349Z"
    },
    "id": "qS67VJG-4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL29OwOV4jnV"
   },
   "source": [
    "### Country by revenue, per year\n",
    "\n",
    "The revenue of a country in a time period, is its total value of orders in that time period. Compute and visualise all countries by revenue, for years 2016, 2017 and 2018. Your visualisation should have countries on the x-axis and multiple bars (one for each year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:18.597583Z",
     "start_time": "2021-10-31T14:39:18.594419Z"
    },
    "id": "_KHGZs0O4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIwka6bY4jnV"
   },
   "source": [
    "### Orders per quarter, all companies\n",
    "\n",
    "Compute and visualise the number of orders each company has placed in each quarter. Exclude quarters where the order count is less than 3. As always, be mindful to not produce a cluttered visualisation.\n",
    "\n",
    "Part of your query should form a variable that converts `date` into `YEAR_QUARTER` format. Dealing with dates is via `STRFTIME` [docs](https://www.sqlite.org/lang_datefunc.html) which doesn't allow quarter extraction. Instead, it allows for extraction of month, which you can case on in order to produce the quarter (Q1, Q2, Q3, Q4).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:23.632721Z",
     "start_time": "2021-10-31T14:39:23.629720Z"
    },
    "id": "zGT4oqVl4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW0LZ7_b4jnV"
   },
   "source": [
    "## Part 4: Parts and prices (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM6CcJQj4jnV"
   },
   "source": [
    "### Parts demand changes\n",
    "\n",
    "A different amount of orders are placed on parts each year. The demand of a part is the number of orders placed on it. The demand change of a part is the absolute difference between its average demand in 2016/2017, and its demand in 2018.\n",
    "\n",
    "Compute and visualise the 15 parts whose demand change has been the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:30.418442Z",
     "start_time": "2021-10-31T14:39:30.415270Z"
    },
    "id": "qjh6pszu4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNGVkZIL4jnV"
   },
   "source": [
    "### Popular parts pricing\n",
    "\n",
    "The most popular parts are those whose demand has increased the most from its 2016/2017 average to 2018. We're interested to find out if popularity is due to a price drop, and also inform us if prices of these parts are properly adjusted.\n",
    "\n",
    "The demand increase of a part is its 2018 demand minus its 2016/2017 average demand. The price change of a part is its average 2018 price minus its average 2016/2017 price.\n",
    "\n",
    "Compute the parts whose demand has increased (has positive demand increase) and the change in price for each of these parts. Then visualise this relationship and include in the figure title the correlation (compute via pandas) between these two variables. Conclude which is most likely 1) Parts became more popular from a drop in prices, or 2) The sales department deserved its bonuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:38.813945Z",
     "start_time": "2021-10-31T14:39:38.811979Z"
    },
    "id": "prx6EWk04jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTYM02c14jnW"
   },
   "source": [
    "*your explanation goes here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02807_Project_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
